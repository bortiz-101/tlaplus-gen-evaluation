{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Generation for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/media/mujtaba/DATA/nick/UnitTestExamples/data/prompts/humanevalx/zero_shot_first_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                                             prompt\n",
       "0             0           0  //Source method\\nimport java.util.*;\\nimport j...\n",
       "1             1           1  //Source method\\nimport java.util.*;\\nimport j...\n",
       "2             2           2  //Source method\\nimport java.util.*;\\nimport j...\n",
       "3             3           3  //Source method\\nimport java.util.*;\\nimport j...\n",
       "4             4           4  //Source method\\nimport java.util.*;\\nimport j..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = Dataset.from_csv(\"../../../data/prompts/humanevalx/first_formatted_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'original_index': 0,\n",
       " 'prompt': '\\nimport java.util.*;\\nimport java.lang.*;\\n\\nclass Solution {\\n    public boolean hasCloseElements(List<Double> numbers, double threshold) {\\n        for (int i = 0; i < numbers.size(); i++) {\\n            for (int j = i + 1; j < numbers.size(); j++) {\\n                double distance = Math.abs(numbers.get(i) - numbers.get(j));\\n                if (distance < threshold) return true;\\n            }\\n        }\\n        return false;\\n    }\\n}\\n\\n//Create Java unit tests for the Java method given above\\n        \\npublic class Main {\\n    public static void main(String[] args) {\\n        '}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = load_dataset(\"THUDM/humaneval-x\", \"java\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Given Attribute:\n",
    "* task_id: original location in dataset\n",
    "* prompt: imports and class declaration + instructions/information for desired function\n",
    "* declaration: imports, class, and method decalaration\n",
    "* canonical_solution: desired code to finish method with no class or method declaration before (continuation after declaration)\n",
    "* test: hidden valid test cases that verify the solution works properly\n",
    "* example_test: public (in prompt) test cases written as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the 10 Shortest Full Solutions from HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_smallest_strings(k, list):\n",
    "    smallest = []\n",
    "    code = ''\n",
    "\n",
    "    for i, example in enumerate(list):\n",
    "        code = example['declaration'] + example['canonical_solution']\n",
    "        if len(smallest) < k:\n",
    "            add_code((i,code), smallest)\n",
    "        elif len(code) < len(smallest[-1][1]):\n",
    "            smallest.pop()\n",
    "            add_code((i,code), smallest)\n",
    "    return smallest\n",
    "\n",
    "def add_code(code, list):\n",
    "    for i in range(len(list)):\n",
    "        if len(code[1]) < len(list[i][1]):\n",
    "            list.insert(i, code)\n",
    "            #print(f\"Inserted this: {code}\")\n",
    "            return\n",
    "    list.append(code)\n",
    "    #print(f\"Appended this: {code}\")\n",
    "\n",
    "ten_smallest = k_smallest_strings(10, ds['test'])\n",
    "\n",
    "for ex in ten_smallest:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best/Simplest Methods for Reference:\n",
    "* 53: add (two int)\n",
    "* 23: strlen (one string)\n",
    "* 45: traingleArea (two double)\n",
    "* 2: truncateNumber (one double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import PartialState\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-770m\"\n",
    "device = \"cuda:1\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# inputs = tokenizer.encode(ds[2]['prompt'], return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, num_beams = 4, do_sample = True, max_length=1000)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    #quantization_config=bnb_config, #Model quantization\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, #Automatic with quantized models\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    #quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Save model generation values\n",
    "gen_config = model.generation_config\n",
    "config_dict = gen_config.to_dict()\n",
    "with open(\"generation_config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''Sum the elements of an array and return the sum with unit tests following the ZOMBIES testing methodology.\n",
    "Here is a breakdown of the ZOMBIES testing methodology:\n",
    "Z - Zero\n",
    "O - One\n",
    "M - Many\n",
    "B - Boundaries\n",
    "I - Interface\n",
    "E - Exceptions\n",
    "S - Simplicity\n",
    "\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "class Solution {\n",
    "    public double sumArray(List<Double> numbers) {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''//Generate Unit Tests.\n",
    "\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\n",
    "/* Example Test       \n",
    "    @Test\n",
    "    void shouldReturnTrueIfHasClosestElementExample1() {\n",
    "        // Arrange\n",
    "        final var s = new Solution();\n",
    "        final var arg1 = List.of(11.0, 2.0, 3.9, 4.0, 5.0, 2.2);\n",
    "        final var arg2 = 0.3;\n",
    "        final var expected = true;\n",
    "        // Act\n",
    "        final var actual = s.hasCloseElements(arg1, arg2);\n",
    "        \n",
    "        // Assert\n",
    "        assertEquals(expected, actual);\n",
    "    }\n",
    "*/\n",
    "\n",
    "//Include tests for empty and singleton Lists.\n",
    "class SolutionTest {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Solution\",\n",
    "    #package_name=\"com.example.calculator\",\n",
    "    dependencies=[\"JUnit 5\", \"Mockito\"],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Generate Unit Tests.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"\"\"\n",
    "        @Test\n",
    "        void shouldReturnTrueIfHasClosestElementExample1() {\n",
    "            // Arrange\n",
    "            final var s = new Solution();\n",
    "            final var arg1 = List.of(11.0, 2.0, 3.9, 4.0, 5.0, 2.2);\n",
    "            final var arg2 = 0.3;\n",
    "            final var expected = true;\n",
    "\n",
    "            // Act\n",
    "            final var actual = s.hasCloseElements(arg1, arg2);\n",
    "            \n",
    "            // Assert\n",
    "            assertEquals(expected, actual);\n",
    "        }\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "package com.example.calculator;\n",
    "\n",
    "public class Calculator {\n",
    "    public int add(int a, int b) {\n",
    "        return a + b;\n",
    "    }\n",
    "    \n",
    "    public int divide(int numerator, int denominator) {\n",
    "        if (denominator == 0) {\n",
    "            throw new ArithmeticException(\"Cannot divide by zero\");\n",
    "        }\n",
    "        return numerator / denominator;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Calculator\",\n",
    "    package_name=\"com.example.calculator\",\n",
    "    dependencies=[\"JUnit 5\", \"Mockito\"],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Test both the add() and divide() methods\n",
    "    - For divide(), include tests for the divide-by-zero exception\n",
    "    - Test with both positive and negative numbers\n",
    "    - Test with boundary values\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"\"\"\n",
    "        @Test\n",
    "        void shouldReturnSumWhenAddingTwoPositiveNumbers() {\n",
    "            // Arrange\n",
    "            Calculator calculator = new Calculator();\n",
    "            \n",
    "            // Act\n",
    "            int result = calculator.add(3, 5);\n",
    "            \n",
    "            // Assert\n",
    "            assertEquals(8, result, \"3 + 5 should equal 8\");\n",
    "        }\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_prompt)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strlen Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "\n",
    "class Solution {\n",
    "    public int strlen(String string) {\n",
    "        return string.length();\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Solution\",\n",
    "    #package_name=\"com.example.calculator\",\n",
    "    dependencies=[],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Generate Unit Tests.\n",
    "    - Test for Empty Cases\n",
    "    - Test for Failing Cases\n",
    "    - Test for Error Causing Cases\n",
    "    \"\"\",\n",
    "    examples=[],\n",
    "    test_framework = \"Any\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['declaration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['canonical_solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['test'][23]['prompt'] + ds['test'][23]['canonical_solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt + \"\\n\\n\\n//Source method\\n\" + remove_block_comments(ds['test'][23]['prompt']) + ds['test'][23]['canonical_solution'] + \"\\n\\n//Unit Tests with Three Cases\\n\" + \"public class Main {\\n    public static void main(String[] args) {\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"//Source method\\n\" + remove_block_comments(ds['test'][45]['prompt']) + ds['test'][45]['canonical_solution'] + \"\\n\\n//Unit Tests with Three Cases\\n\" + ds['test'][45]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"//Source method\\nClass Solution{\" + ds['test'][45]['canonical_solution'] + \"\\n\\n//Unit Tests with Three Cases\\n\" + ds['test'][45]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_block_comments(java_code):\n",
    "    \"\"\"\n",
    "    Removes all block comments (/* ... */) from the given Java code.\n",
    "    \n",
    "    Args:\n",
    "        java_code (str): The Java code as a string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The Java code with block comments removed.\n",
    "    \"\"\"\n",
    "    # Regex pattern to match block comments\n",
    "    pattern = re.compile(r'/\\*.*?\\*/', re.DOTALL)  # re.DOTALL makes .* match newlines\n",
    "    \n",
    "    # Substitute block comments with empty string\n",
    "    cleaned_code = re.sub(pattern, '', java_code)\n",
    "    \n",
    "    return cleaned_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_block_comments(java_code):\n",
    "    pattern = re.compile(r'/\\*.*?\\*/\\s*\\n?', re.DOTALL)  # Finds block and newline character after\n",
    "    \n",
    "    # Substitute block comments (and following newline) with empty string\n",
    "    cleaned_code = re.sub(pattern, '', java_code)\n",
    "    \n",
    "    return cleaned_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_block_comments(ds['test'][23]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public double triangleArea(double a, double h) {\n",
      "        return a * h / 2;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Tests with Three Cases\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.triangleArea(5, 3) == 7.5,\n",
      "                s.triangleArea(2, 2) == 2.0,\n",
      "                s.triangleArea(10, 8) == 40.0\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Tests with Three Cases\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.strlen(\"Hello\") == 5,\n",
      "                s.strlen(\"World\") == 5,\n",
      "                s.strlen(\"Java\") == 4\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Tests with Three Cases\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.strlen(\"Hello\") == 5,\n",
      "                s.strlen(\"World\") == 5,\n",
      "                s.strlen(\"Java\") == 4\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Tests with Three Cases\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.strlen\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "outputs = model.generate(inputs, max_new_tokens=300)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "outputs = model.generate(inputs, \n",
    "                         \n",
    "                         ####OUTPUT LENGTH CONTROL####\n",
    "                         #max_length = 1200, #optional, default = 20, max length of tokens in input prompt + newly generated tokens\n",
    "                         max_new_tokens = 300, #optional, max number of model generated tokens (not including prompt)\n",
    "                         min_length = 0, #optional, default = 0, min length of tokens in input prompt + newly generated tokens\n",
    "                         #min_new_tokens, #optional, min number of model generated tokens (not including prompt)\n",
    "                         early_stopping = False, #optional, default = False, True: stops with num_beams completion candidates; False: Stops when better candidates are unlikely; \"never\": only stops when there cannot be any better candidates\n",
    "                         max_time = 120, #optional (in seconds)\n",
    "                         #stop_strings, #optional (string or list of strings that terminate generation)\n",
    "                         \n",
    "                         ####MANIPULATION OF MODEL OUTPUT LOGITS####\n",
    "                         temperature = 1.5, #optional, default = 1.0, >1.0: more random; <1.0: more controlled/repetitive; 1.0: balance between both (check generation.config file for default values)\n",
    "                         top_k = 50, #optional, default = 50, considers the k most probable tokens for generation\n",
    "                         top_p = 1, #optional, default = 1.0, cumulative probability of all top_k tokens needed; if set <1.0, only the number of tokens that achieve that probability are considered (check generation.config for alterations)\n",
    "                         #min_p = 0.01, #optional, minimum token probability for consideration, scaled based on the most likely token\n",
    "                         typical_p = 1.0, #optional, default = 1.0, ?\n",
    "                         epsilon_cutoff = 0.0, #optional, default = 0.0, tokens must have a conditional probability greater than the value (3e-4 - 9e-4)\n",
    "                         eta_cutoff = 0.0, #optional, default = 0.0, hybrid between epsilon and typical p sampling (3e-4 - 2e-3)\n",
    "                         diversity_penalty = 2.0, #optional, default = 0.0, value subtracted from beam score if another beam generates the same token; group beam search must be enabled\n",
    "                         repetition_penalty = 10.0, #optional, default = 1.0, 1.0: no penalty, ****need to experiment with values****\n",
    "                         #encoder_repitition_penalty = 1.0, #optional, default = 1.0, exponential penalty on sequences not in the original input\n",
    "                         length_penalty = 0.0, #optional, default = 1.0, exponential penalty to length; <0.0 means shorter sequences; >0.0 means longer sequences\n",
    "                         no_repeat_ngram_size = 0, #optional, default = 0, if set, ngrams of that size can never be repeated once used\n",
    "                         #bad_words_ids, #optional, tokens that are prevented from being generated\n",
    "                         #force_word_ids, #optional, list of token ids that must be generated\n",
    "                         renormalize_logits = True, #optional, default = False, whether logits are renormalized,recommended to be set to True as logit processors can break normalization\n",
    "                         #constraints, #optional, list of constraints added to generation to promote usage of certain tokens\n",
    "                         forced_bos_token_id = model.config.forced_bos_token_id, #optional, defaults to model.config.forced_bos_token_id, this is the token forced to be the first generation\n",
    "                         forced_eos_token_id = model.config.forced_eos_token_id, #optional, default = model.config.forced_eos_token_id, this is the token forced to be the last generation\n",
    "                         remove_invalid_values = model.config.remove_invalid_values, #optional, defaults to model.config.remove_invalid_values, if true, removes nan and inf outputs to prevent crash, but slows down generation\n",
    "                         #exponential_decay_length_penalty, #optional, given tuple that adds increasing penalty after the given number of tokens are generated tuple(int(num tokens), float(penalty))\n",
    "                         #suppress_tokens, #optional, list of tokens suppressed at generation; log probs set to inf\n",
    "                         #begin_suppress_tokens, #optional, list of tokens suppressed at beginning of generation; log probs set to inf\n",
    "                         #forced_decoder_ids, #optional, list of generation indicies to token indicies that will be forced before sampling begins\n",
    "                         #sequence_bias, #optional, dictionary mapping sequences to bias terms; positive bias = increased odds, negative bias = decreased odds\n",
    "                         token_healing = False, #optional, default = False\n",
    "                         #guidance_scale, #optional\n",
    "                         #low_memory, #optional\n",
    "                         #watermarking_config, #optional\n",
    "\n",
    "                         ####GENERATION STRATEGY####\n",
    "                         do_sample = False, #optional, default = False, whether sampling is used; greedy decoding if False\n",
    "                         num_beams = 8, #optional, default = 1, number of beams used for beam search; 1 means no beam search\n",
    "                         num_beam_groups = 4, #optional, default = 1, number of beam groups that beams divide into to ensure diversity among beam groups\n",
    "                         #penalty_alpha, #optional, balances model confidence and degeneration penalty in contrastive search decoding\n",
    "                         #dola_layers, #optional, number of layers used for DoLa decoding (LOOK INTO MORE THROUGH DOCS AND PAPER)\n",
    "\n",
    "                         ####SPECIAL TOKENS AT GENERATION TIME####\n",
    "                         pad_token_id = tokenizer.eos_token_id, #optional, id of padding token\n",
    "                         #bos_token_id, #optional, id of beginning-of-sequence token\n",
    "                         #eos_token_id, #optional, id of end-of-sequence token\n",
    "\n",
    "                         ####CACHE CONTROL####\n",
    "                         use_cache = True, #optional, default = True, whether model uses past key/values attentions to speed up decoding\n",
    "                         cache_implementation = None, #optional, default = None, type of cache implementation used\n",
    "                         cache_config = None, #optional, default = None, \n",
    "                         return_legacy_cache = True, #optional, default = True                   \n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SantaCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/santacoder\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCoder2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "device = \"cuda:2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codegen2-1B_P\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(\"//Create a function that sums two numbers together and returns the result\\npublic static int sum(int a, int b){\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitTestGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
