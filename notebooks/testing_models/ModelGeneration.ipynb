{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Generation for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../data/prompts/first_formatted_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = Dataset.from_csv(\"../../../data/prompts/humanevalx/first_formatted_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = load_dataset(\"THUDM/humaneval-x\", \"java\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Given Attribute:\n",
    "* task_id: original location in dataset\n",
    "* prompt: imports and class declaration + instructions/information for desired function\n",
    "* declaration: imports, class, and method decalaration\n",
    "* canonical_solution: desired code to finish method with no class or method declaration before (continuation after declaration)\n",
    "* test: hidden valid test cases that verify the solution works properly\n",
    "* example_test: public (in prompt) test cases written as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the 10 Shortest Full Solutions from HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_smallest_strings(k, list):\n",
    "    smallest = []\n",
    "    code = ''\n",
    "\n",
    "    for i, example in enumerate(list):\n",
    "        code = example['declaration'] + example['canonical_solution']\n",
    "        if len(smallest) < k:\n",
    "            add_code((i,code), smallest)\n",
    "        elif len(code) < len(smallest[-1][1]):\n",
    "            smallest.pop()\n",
    "            add_code((i,code), smallest)\n",
    "    return smallest\n",
    "\n",
    "def add_code(code, list):\n",
    "    for i in range(len(list)):\n",
    "        if len(code[1]) < len(list[i][1]):\n",
    "            list.insert(i, code)\n",
    "            #print(f\"Inserted this: {code}\")\n",
    "            return\n",
    "    list.append(code)\n",
    "    #print(f\"Appended this: {code}\")\n",
    "\n",
    "ten_smallest = k_smallest_strings(10, ds['test'])\n",
    "\n",
    "for ex in ten_smallest:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best/Simplest Methods for Reference:\n",
    "* 53: add (two int)\n",
    "* 23: strlen (one string)\n",
    "* 45: traingleArea (two double)\n",
    "* 2: truncateNumber (one double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import PartialState\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-770m\"\n",
    "device = \"cuda:1\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# inputs = tokenizer.encode(ds[2]['prompt'], return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, num_beams = 4, do_sample = True, max_length=1000)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    quantization_config=bnb_config, #Model quantization\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, #Automatic with quantized models\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    #quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''Sum the elements of an array and return the sum with unit tests following the ZOMBIES testing methodology.\n",
    "Here is a breakdown of the ZOMBIES testing methodology:\n",
    "Z - Zero\n",
    "O - One\n",
    "M - Many\n",
    "B - Boundaries\n",
    "I - Interface\n",
    "E - Exceptions\n",
    "S - Simplicity\n",
    "\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "class Solution {\n",
    "    public double sumArray(List<Double> numbers) {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''//Generate Unit Tests.\n",
    "\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\n",
    "/* Example Test       \n",
    "    @Test\n",
    "    void shouldReturnTrueIfHasClosestElementExample1() {\n",
    "        // Arrange\n",
    "        final var s = new Solution();\n",
    "        final var arg1 = List.of(11.0, 2.0, 3.9, 4.0, 5.0, 2.2);\n",
    "        final var arg2 = 0.3;\n",
    "        final var expected = true;\n",
    "        // Act\n",
    "        final var actual = s.hasCloseElements(arg1, arg2);\n",
    "        \n",
    "        // Assert\n",
    "        assertEquals(expected, actual);\n",
    "    }\n",
    "*/\n",
    "\n",
    "//Include tests for empty and singleton Lists.\n",
    "class SolutionTest {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Solution\",\n",
    "    #package_name=\"com.example.calculator\",\n",
    "    dependencies=[\"JUnit 5\", \"Mockito\"],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Generate Unit Tests.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"\"\"\n",
    "        @Test\n",
    "        void shouldReturnTrueIfHasClosestElementExample1() {\n",
    "            // Arrange\n",
    "            final var s = new Solution();\n",
    "            final var arg1 = List.of(11.0, 2.0, 3.9, 4.0, 5.0, 2.2);\n",
    "            final var arg2 = 0.3;\n",
    "            final var expected = true;\n",
    "\n",
    "            // Act\n",
    "            final var actual = s.hasCloseElements(arg1, arg2);\n",
    "            \n",
    "            // Assert\n",
    "            assertEquals(expected, actual);\n",
    "        }\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "package com.example.calculator;\n",
    "\n",
    "public class Calculator {\n",
    "    public int add(int a, int b) {\n",
    "        return a + b;\n",
    "    }\n",
    "    \n",
    "    public int divide(int numerator, int denominator) {\n",
    "        if (denominator == 0) {\n",
    "            throw new ArithmeticException(\"Cannot divide by zero\");\n",
    "        }\n",
    "        return numerator / denominator;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Calculator\",\n",
    "    package_name=\"com.example.calculator\",\n",
    "    dependencies=[\"JUnit 5\", \"Mockito\"],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Test both the add() and divide() methods\n",
    "    - For divide(), include tests for the divide-by-zero exception\n",
    "    - Test with both positive and negative numbers\n",
    "    - Test with boundary values\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"\"\"\n",
    "        @Test\n",
    "        void shouldReturnSumWhenAddingTwoPositiveNumbers() {\n",
    "            // Arrange\n",
    "            Calculator calculator = new Calculator();\n",
    "            \n",
    "            // Act\n",
    "            int result = calculator.add(3, 5);\n",
    "            \n",
    "            // Assert\n",
    "            assertEquals(8, result, \"3 + 5 should equal 8\");\n",
    "        }\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_prompt)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strlen Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "You are a Java testing expert. Your task is to generate comprehensive tests for the provided code.\n",
      "\n",
      "\n",
      "## Source Code to Test\n",
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "## Class Information\n",
      "- Class name: `Solution`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Testing Requirements\n",
      "\n",
      "\n",
      "    - Generate Unit Tests.\n",
      "    - Test for Empty Cases\n",
      "    - Test for Failing Cases\n",
      "    - Test for Error Causing Cases\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "## Testing Framework\n",
      "Use JUnit 5 with Mockito for writing the tests.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generate a complete test class with all necessary imports, setup, and test methods.\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "\n",
    "class Solution {\n",
    "    public int strlen(String string) {\n",
    "        return string.length();\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Solution\",\n",
    "    #package_name=\"com.example.calculator\",\n",
    "    dependencies=[],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Generate Unit Tests.\n",
    "    - Test for Empty Cases\n",
    "    - Test for Failing Cases\n",
    "    - Test for Error Causing Cases\n",
    "    \"\"\",\n",
    "    examples=[] \n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import java.util.*;\\nimport java.lang.*;\\n\\nclass Solution {\\n    public int strlen(String string) {\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['test'][23]['declaration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        return string.length();\\n    }\\n}'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['test'][23]['canonical_solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public class Main {\\n    public static void main(String[] args) {\\n        Solution s = new Solution();\\n        List<Boolean> correct = Arrays.asList(\\n                s.strlen(\"\") == 0,\\n                s.strlen(\"x\") == 1,\\n                s.strlen(\"asdasnakj\") == 9\\n        );\\n        if (correct.contains(false)) {\\n            throw new AssertionError();\\n        }\\n    }\\n}'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['test'][23]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    /**\n",
      "    Return length of given string\n",
      "    >>> strlen(\"\")\n",
      "    0\n",
      "    >>> strlen(\"abc\")\n",
      "    3\n",
      "     */\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(ds['test'][23]['prompt'] + ds['test'][23]['canonical_solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "You are a Java testing expert. Your task is to generate comprehensive tests for the provided code.\n",
      "\n",
      "\n",
      "## Source Code to Test\n",
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "## Class Information\n",
      "- Class name: `Solution`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Testing Requirements\n",
      "\n",
      "\n",
      "    - Generate Unit Tests.\n",
      "    - Test for Empty Cases\n",
      "    - Test for Failing Cases\n",
      "    - Test for Error Causing Cases\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "## Testing Framework\n",
      "Use JUnit 5 with Mockito for writing the tests.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generate a complete test class with all necessary imports, setup, and test methods.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/src/main/java/com/example/demo/service/UserService.java\n",
      "package com.example.demo.service;\n",
      "\n",
      "import com.example.demo.model.User;\n",
      "import com.example.demo.repository.UserRepository;\n",
      "import org.springframework.beans.factory.annotation.Autowired;\n",
      "import org.springframework.stereotype.Service;\n",
      "\n",
      "import java.util.List;\n",
      "import java.util.Optional;\n",
      "\n",
      "@Service\n",
      "public class UserService {\n",
      "\n",
      "    @Autowired\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    public List<User> getAllUsers() {\n",
      "        return userRepository.findAll();\n",
      "    }\n",
      "\n",
      "    public Optional<User> getUserById(Long id) {\n",
      "        return userRepository.findById(id);\n",
      "    }\n",
      "\n",
      "    public User createUser(User user) {\n",
      "        return userRepository.save(user);\n",
      "    }\n",
      "\n",
      "    public User updateUser(User user) {\n",
      "        return userRepository.save(user);\n",
      "    }\n",
      "\n",
      "    public void deleteUser(Long id) {\n",
      "        userRepository.deleteById(id);\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "outputs = model.generate(inputs, \n",
    "                         \n",
    "                         ####OUTPUT LENGTH CONTROL####\n",
    "                         max_length = 1200, #optional, default = 20, max length of tokens in input prompt + newly generated tokens\n",
    "                         #max_new_tokens = 150, #optional, max number of model generated tokens (not including prompt)\n",
    "                         min_length = 0, #optional, default = 0, min length of tokens in input prompt + newly generated tokens\n",
    "                         #min_new_tokens, #optional, min number of model generated tokens (not including prompt)\n",
    "                         early_stopping = False, #optional, default = False, True: stops with num_beams completion candidates; False: Stops when better candidates are unlikely; \"never\": only stops when there cannot be any better candidates\n",
    "                         max_time = 120, #optional (in seconds)\n",
    "                         #stop_strings, #optional (string or list of strings that terminate generation)\n",
    "                         \n",
    "                         ####MANIPULATION OF MODEL OUTPUT LOGITS####\n",
    "                         temperature = 0.3, #optional, default = 1.0, >1.0: more random; <1.0: more controlled/repetitive; 1.0: balance between both (check generation.config file for default values)\n",
    "                         top_k = 50, #optional, default = 50, considers the k most probable tokens for generation\n",
    "                         top_p = 0.95, #optional, default = 1.0, cumulative probability of all top_k tokens needed; if set <1.0, only the number of tokens that achieve that probability are considered (check generation.config for alterations)\n",
    "                         #min_p = 0.01, #optional, minimum token probability for consideration, scaled based on the most likely token\n",
    "                         typical_p = 1.0, #optional, default = 1.0, ?\n",
    "                         epsilon_cutoff = 0.0, #optional, default = 0.0, tokens must have a conditional probability greater than the value (3e-4 - 9e-4)\n",
    "                         eta_cutoff = 0.0, #optional, default = 0.0, hybrid between epsilon and typical p sampling (3e-4 - 2e-3)\n",
    "                         diversity_penalty = 1.0, #optional, default = 0.0, value subtracted from beam score if another beam generates the same token; group beam search must be enabled\n",
    "                         repetition_penalty = 2.0, #optional, default = 1.0, 1.0: no penalty, ****need to experiment with values****\n",
    "                         #encoder_repitition_penalty = 1.0, #optional, default = 1.0, exponential penalty on sequences not in the original input\n",
    "                         length_penalty = 0.0, #optional, default = 1.0, exponential penalty to length; <0.0 means shorter sequences; >0.0 means longer sequences\n",
    "                         no_repeat_ngram_size = 0, #optional, default = 0, if set, ngrams of that size can never be repeated once used\n",
    "                         #bad_words_ids, #optional, tokens that are prevented from being generated\n",
    "                         #force_word_ids, #optional, list of token ids that must be generated\n",
    "                         renormalize_logits = True, #optional, default = False, whether logits are renormalized,recommended to be set to True as logit processors can break normalization\n",
    "                         #constraints, #optional, list of constraints added to generation to promote usage of certain tokens\n",
    "                         forced_bos_token_id = model.config.forced_bos_token_id, #optional, defaults to model.config.forced_bos_token_id, this is the token forced to be the first generation\n",
    "                         forced_eos_token_id = model.config.forced_eos_token_id, #optional, default = model.config.forced_eos_token_id, this is the token forced to be the last generation\n",
    "                         remove_invalid_values = model.config.remove_invalid_values, #optional, defaults to model.config.remove_invalid_values, if true, removes nan and inf outputs to prevent crash, but slows down generation\n",
    "                         #exponential_decay_length_penalty, #optional, given tuple that adds increasing penalty after the given number of tokens are generated tuple(int(num tokens), float(penalty))\n",
    "                         #suppress_tokens, #optional, list of tokens suppressed at generation; log probs set to inf\n",
    "                         #begin_suppress_tokens, #optional, list of tokens suppressed at beginning of generation; log probs set to inf\n",
    "                         #forced_decoder_ids, #optional, list of generation indicies to token indicies that will be forced before sampling begins\n",
    "                         #sequence_bias, #optional, dictionary mapping sequences to bias terms; positive bias = increased odds, negative bias = decreased odds\n",
    "                         token_healing = False, #optional, default = False\n",
    "                         #guidance_scale, #optional\n",
    "                         #low_memory, #optional\n",
    "                         #watermarking_config, #optional\n",
    "\n",
    "                         ####GENERATION STRATEGY####\n",
    "                         do_sample = False, #optional, default = False, whether sampling is used; greedy decoding if False\n",
    "                         num_beams = 8, #optional, default = 1, number of beams used for beam search; 1 means no beam search\n",
    "                         num_beam_groups = 4, #optional, default = 1, number of beam groups that beams divide into to ensure diversity among beam groups\n",
    "                         #penalty_alpha, #optional, balances model confidence and degeneration penalty in contrastive search decoding\n",
    "                         #dola_layers, #optional, number of layers used for DoLa decoding (LOOK INTO MORE THROUGH DOCS AND PAPER)\n",
    "\n",
    "                         ####SPECIAL TOKENS AT GENERATION TIME####\n",
    "                         pad_token_id = tokenizer.eos_token_id, #optional, id of padding token\n",
    "                         #bos_token_id, #optional, id of beginning-of-sequence token\n",
    "                         #eos_token_id, #optional, id of end-of-sequence token\n",
    "\n",
    "                         ####CACHE CONTROL####\n",
    "                         use_cache = True, #optional, default = True, whether model uses past key/values attentions to speed up decoding\n",
    "                         cache_implementation = None, #optional, default = None, type of cache implementation used\n",
    "                         cache_config = None, #optional, default = None, \n",
    "                         return_legacy_cache = True, #optional, default = True                   \n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SantaCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/santacoder\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCoder2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "device = \"cuda:2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codegen2-1B_P\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(\"//Create a function that sums two numbers together and returns the result\\npublic static int sum(int a, int b){\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitTestGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
