{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Generation for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../data/prompts/first_formatted_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = Dataset.from_csv(\"../../../data/prompts/humanevalx/first_formatted_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        for (int i = 0; i < numbers.size(); i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
      "                if (distance < threshold) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Create Java unit tests for the Java method given above\n",
      "        \n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(ds[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"//Create a function that sums two numbers together and returns the result\\npublic static int sum(int a, int b){\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeT5p-770m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Solution s = new Solution();\n",
      "        System.out.println(s.truncateNumber(1.0));\n",
      "        System.out.println(s.truncateNumber(2.0));\n",
      "        System.out.println(s.truncateNumber(3.0));\n",
      "        System.out.println(s.truncateNumber(4.0));\n",
      "        System.out.println(s.truncateNumber(5.0));\n",
      "        System.out.println(s.truncateNumber(6.0));\n",
      "        System.out.println(s.truncateNumber(7.0));\n",
      "        System.out.println(s.truncateNumber(8.0));\n",
      "        System.out.println(s.truncateNumber(9.0));\n",
      "        System.out.println(s.truncateNumber(10.0));\n",
      "        System.out.println(s.truncateNumber(11.0));\n",
      "        System.out.println(s.truncateNumber(12.0));\n",
      "        System.out.println(s.truncateNumber(13.0));\n",
      "        System.out.println(s.truncateNumber(14.0));\n",
      "        System.out.println(s.truncateNumber(15.0));\n",
      "        System.out.println(s.truncateNumber(16.0));\n",
      "        System.out.println(s.truncateNumber(17.0));\n",
      "        System.out.println(s.truncateNumber(18.0));\n",
      "        System.out.println(s.truncateNumber(19.0));\n",
      "        System.out.println(s.truncateNumber(20.0));\n",
      "        System.out.println(s.truncateNumber(21.0));\n",
      "        System.out.println(s.truncateNumber(22.0));\n",
      "        System.out.println(s.truncateNumber(23.0));\n",
      "        System.out.println(s.truncateNumber(24.0));\n",
      "        System.out.println(s.truncateNumber(25.0));\n",
      "        System.out.println(s.truncateNumber(26.0));\n",
      "        System.out.println(s.truncateNumber(27.0));\n",
      "        System.out.println(s.truncateNumber(28.0));\n",
      "        System.out.println(s.truncateNumber(29.0));\n",
      "        System.out.println(s.truncateNumber(30.0));\n",
      "        System.out.println(s.truncateNumber(31.0));\n",
      "        System.out.println(s.truncateNumber(32.0));\n",
      "        System.out.println(s.truncateNumber(33.0));\n",
      "        System.out.println(s.truncateNumber(34.0));\n",
      "        System.out.println(s.truncateNumber(35.0));\n",
      "        System.out.println(s.truncateNumber(36.0));\n",
      "        System.out.println(s.truncateNumber(37.0));\n",
      "        System.out.println(s.truncateNumber(38.0));\n",
      "        System.out.println(s.truncateNumber(39.0));\n",
      "        System.out.println(s.truncateNumber(40.0));\n",
      "        System.out.println(s.truncateNumber(41.0));\n",
      "        System.out.println(s.truncateNumber(42.0));\n",
      "        System.out.println(s.truncateNumber(43.0));\n",
      "        System.out.println(s.truncateNumber(44.0));\n",
      "        System.out.println(s.truncateNumber(45.0));\n",
      "        System.out.println(s.truncateNumber(46.0));\n",
      "        System.out.println(s.truncateNumber(47.0));\n",
      "        System.out.println(s.truncateNumber(48.0));\n",
      "        System.out.println(s.truncateNumber(49.0));\n",
      "        System.out.println(s.truncateNumber(50.0));\n",
      "        System.out.println(s.truncateNumber(51.0));\n",
      "        System.out.println(s.truncateNumber(52.0));\n",
      "        System.out.println(s.truncateNumber(53.0));\n",
      "        System.out.println(s.truncateNumber(54.0));\n",
      "        System.out.println(s.truncateNumber(55.0));\n",
      "        System.out.println(s.truncateNumber(56.0));\n",
      "        System.out.println(s.truncateNumber(57.0));\n",
      "        System.out.println(s.truncateNumber(58.0));\n",
      "        System.out\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-770m\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(ds[2]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, num_beams = 4, do_sample = True, max_length=1000)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# ==> print \"Hello World\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''Sum the elements of an array and return the sum with unit tests following the ZOMBIES testing methodology.\n",
    "Here is a breakdown of the ZOMBIES testing methodology:\n",
    "Z - Zero\n",
    "O - One\n",
    "M - Many\n",
    "B - Boundaries\n",
    "I - Interface\n",
    "E - Exceptions\n",
    "S - Simplicity\n",
    "\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "class Solution {\n",
    "    public double sumArray(List<Double> numbers) {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''Generate Unit Tests.\n",
    "\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "        \n",
    "public class SolutionTest {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool: \n",
      "\"\"\" Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True \"\"\"\n"
     ]
    }
   ],
   "source": [
    "prompt = 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool: \\n\"\"\" Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True \"\"\"'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import PartialState\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Unit Tests.\n",
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "import static org.junit.jupiter.api.Assertions.*;\n",
      "import org.junit.jupiter.api.Test;\n",
      "\n",
      "\n",
      "class Solution {\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        for (int i = 0; i < numbers.size(); i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
      "                if (distance < threshold) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "        \n",
      "public class SolutionTest {\n",
      "    @Test\n",
      "    public void test1() {\n",
      "        Solution solution = new Solution();\n",
      "        List<Double> numbers = new ArrayList<>();\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         num_beams = 2, \n",
    "                         num_beam_groups = 2,\n",
    "                         do_sample = False, \n",
    "                         max_new_tokens = 500,\n",
    "                         top_p = 1,\n",
    "                         top_k = 50,\n",
    "                         #penalty_alpha = 0.5,\n",
    "                         length_penalty = 1.0,\n",
    "                         diversity_penalty = 0.5,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcodellama/CodeLlama-7b-Instruct-hf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcodellama/CodeLlama-7b-Instruct-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mbnb_config, \u001b[39m#Model quantization\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m#Automatic with quantized models\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/modeling_utils.py:4245\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4235\u001b[0m         load_contexts\u001b[39m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4237\u001b[0m     \u001b[39mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4238\u001b[0m         (\n\u001b[1;32m   4239\u001b[0m             model,\n\u001b[1;32m   4240\u001b[0m             missing_keys,\n\u001b[1;32m   4241\u001b[0m             unexpected_keys,\n\u001b[1;32m   4242\u001b[0m             mismatched_keys,\n\u001b[1;32m   4243\u001b[0m             offload_index,\n\u001b[1;32m   4244\u001b[0m             error_msgs,\n\u001b[0;32m-> 4245\u001b[0m         ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   4246\u001b[0m             model,\n\u001b[1;32m   4247\u001b[0m             state_dict,\n\u001b[1;32m   4248\u001b[0m             loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4249\u001b[0m             resolved_archive_file,\n\u001b[1;32m   4250\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   4251\u001b[0m             ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   4252\u001b[0m             sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   4253\u001b[0m             _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   4254\u001b[0m             low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   4255\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4256\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4257\u001b[0m             offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   4258\u001b[0m             dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   4259\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4260\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4261\u001b[0m             gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[1;32m   4262\u001b[0m             weights_only\u001b[39m=\u001b[39;49mweights_only,\n\u001b[1;32m   4263\u001b[0m         )\n\u001b[1;32m   4265\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4266\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/modeling_utils.py:4815\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4814\u001b[0m         fixed_state_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4815\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4816\u001b[0m             model_to_load,\n\u001b[1;32m   4817\u001b[0m             fixed_state_dict,\n\u001b[1;32m   4818\u001b[0m             start_prefix,\n\u001b[1;32m   4819\u001b[0m             expected_keys,\n\u001b[1;32m   4820\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4821\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4822\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   4823\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   4824\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   4825\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   4826\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4827\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   4828\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4829\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   4830\u001b[0m         )\n\u001b[1;32m   4831\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   4832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4833\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/modeling_utils.py:873\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    870\u001b[0m         param_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_local_dist_rank_0() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mset_module_kwargs)\n\u001b[1;32m    874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m param_cls(new_value, requires_grad\u001b[39m=\u001b[39mold_value\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    quantization_config=bnb_config, #Model quantization\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, #Automatic with quantized models\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelGeneration.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Unit Tests.\n",
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "import static org.junit.jupiter.api.Assertions.*;\n",
      "import org.junit.jupiter.api.Test;\n",
      "\n",
      "\n",
      "class Solution {\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        for (int i = 0; i < numbers.size(); i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
      "                if (distance < threshold) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "        \n",
      "public class SolutionTest {\n",
      "    @Test\n",
      "    public\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda:1')\n",
    "outputs = model.generate(inputs, \n",
    "                         \n",
    "                         ####OUTPUT LENGTH CONTROL####\n",
    "                         max_length = 20, #optional, default = 20\n",
    "                         max_new_tokens = 500, #optional\n",
    "                         min_length = 0, #optional, default = 0\n",
    "                         #min_new_tokens, #optional\n",
    "                         early_stopping = False, #optional, default = False\n",
    "                         max_time = 120, #optional (in seconds)\n",
    "                         #stop_strings, #optional (string or list of strings to terminate generation)\n",
    "                         \n",
    "                         ####MANIPULATION OF MODEL OUTPUT LOGITS####\n",
    "                         temperature = 1.0, #optional, default = 1.0 \n",
    "                         top_k = 50, #optional, default = 50\n",
    "                         top_p = 1.0, #optional, default = 1.0\n",
    "                         #min_p = 0.01, #optional\n",
    "                         typical_p = 1.0, #optional, default = 1.0\n",
    "                         epsilon_cutoff = 0.0, #optional, default = 0.0\n",
    "                         eta_cutoff = 0.0, #optional, default = 0.0\n",
    "                         diversity_penalty = 0.0, #optional, default = 0.0\n",
    "                         repetition_penalty = 1.0, #optional, default = 1.0\n",
    "                         #encoder_repitition_penalty = 1.0, #optional, default = 1.0\n",
    "                         length_penalty = 1.0, #optional, default = 1.0\n",
    "                         no_repeat_ngram_size = 0, #optional, default = 0\n",
    "                         #bad_words_ids, #optional\n",
    "                         #force_word_ids, #optional\n",
    "                         renormalize_logits = False, #optional, default = False\n",
    "                         #constraints, #optional\n",
    "                         forced_bos_token_id = model.config.forced_bos_token_id, #optional, defaults to model.config.forced_bos_token_id\n",
    "                         forced_eos_token_id = model.config.forced_eos_token_id, #optional, default = model.config.forced_eos_token_id\n",
    "                         remove_invalid_values = model.config.remove_invalid_values, #optional, defaults to model.config.remove_invalid_values\n",
    "                         #exponential_decay_length_penalty, #optional\n",
    "                         #suppress_tokens, #optional\n",
    "                         #begin_suppress_tokens, #optional\n",
    "                         #forced_decoder_ids, #optional\n",
    "                         #sequence_bias, #optional\n",
    "                         token_healing = False, #optional, default = False\n",
    "                         #guidance_scale, #optional\n",
    "                         #low_memory, #optional\n",
    "                         #watermarking_config, #optional\n",
    "\n",
    "                         ####GENERATION STRATEGY####\n",
    "                         do_sample = False, #optional, default = False\n",
    "                         num_beams = 1, #optional, default = 1\n",
    "                         num_beam_groups = 1, #optional, default = 1\n",
    "                         #penalty_alpha, #optional\n",
    "                         #dola_layers, #optional\n",
    "\n",
    "                         ####SPECIAL TOKENS AT GENERATION TIME####\n",
    "                         #pad_token_id, #optional\n",
    "                         #bos_token_id, #optional\n",
    "                         #eos_token_id, #optional\n",
    "\n",
    "                         ####CACHE CONTROL####\n",
    "                         use_cache = True, #optional, default = True\n",
    "                         cache_implementation = None, #optional, default = None\n",
    "                         cache_config = None, #optional, default = None\n",
    "                         return_legacy_cache = True, #optional, default = True                   \n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SantaCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT2CustomModel' object has no attribute '_attn_implementation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(test_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:824\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    821\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m _use_sdpa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    825\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2CustomModel' object has no attribute '_attn_implementation'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/santacoder\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCoder2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        for (int i = 0; i < numbers.size(); i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
      "                if (distance < threshold) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Create Java unit tests for the Java method given above\n",
      "        \n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "         Solution solution = new Solution();\n",
      "         List<Double> numbers = new ArrayList<>();\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "device = \"cuda:2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        for (int i = 0; i < numbers.size(); i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
      "                if (distance < threshold) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Create Java unit tests for the Java method given above\n",
      "        \n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "         Solution solution = new Solution();\n",
      "         List<Double> numbers = new ArrayList<>();\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1.0);\n",
      "         numbers.add(1\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/nzappia/.cache/huggingface/modules/transformers_modules/Salesforce/codegen2-1B_P/47f05cd3c1b357745fbead74202fc1efe87b6d25/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647380992/work/aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Create a function that sums two numbers together and returns the result\n",
      "public static int sum(int a, int b){\n",
      "    return a + b;\n",
      "}\n",
      "\n",
      "//Create a function that multiplies two numbers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codegen2-1B_P\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(\"//Create a function that sums two numbers together and returns the result\\npublic static int sum(int a, int b){\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitTestGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
