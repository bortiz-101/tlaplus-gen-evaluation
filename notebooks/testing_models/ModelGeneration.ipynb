{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Generation for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/media/mujtaba/DATA/nick/UnitTestExamples/data/prompts/humanevalx/zero_shot_first_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>//Source method\\nimport java.util.*;\\nimport j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                                             prompt\n",
       "0             0           0  //Source method\\nimport java.util.*;\\nimport j...\n",
       "1             1           1  //Source method\\nimport java.util.*;\\nimport j...\n",
       "2             2           2  //Source method\\nimport java.util.*;\\nimport j...\n",
       "3             3           3  //Source method\\nimport java.util.*;\\nimport j...\n",
       "4             4           4  //Source method\\nimport java.util.*;\\nimport j..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = Dataset.from_csv(\"../../../data/prompts/humanevalx/first_formatted_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'original_index': 0,\n",
       " 'prompt': '\\nimport java.util.*;\\nimport java.lang.*;\\n\\nclass Solution {\\n    public boolean hasCloseElements(List<Double> numbers, double threshold) {\\n        for (int i = 0; i < numbers.size(); i++) {\\n            for (int j = i + 1; j < numbers.size(); j++) {\\n                double distance = Math.abs(numbers.get(i) - numbers.get(j));\\n                if (distance < threshold) return true;\\n            }\\n        }\\n        return false;\\n    }\\n}\\n\\n//Create Java unit tests for the Java method given above\\n        \\npublic class Main {\\n    public static void main(String[] args) {\\n        '}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "ds = load_dataset(\"THUDM/humaneval-x\", \"java\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Given Attribute:\n",
    "* task_id: original location in dataset\n",
    "* prompt: imports and class declaration + instructions/information for desired function\n",
    "* declaration: imports, class, and method decalaration\n",
    "* canonical_solution: desired code to finish method with no class or method declaration before (continuation after declaration)\n",
    "* test: hidden valid test cases that verify the solution works properly\n",
    "* example_test: public (in prompt) test cases written as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the 10 Shortest Full Solutions from HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_smallest_strings(k, list):\n",
    "    smallest = []\n",
    "    code = ''\n",
    "\n",
    "    for i, example in enumerate(list):\n",
    "        code = example['declaration'] + example['canonical_solution']\n",
    "        if len(smallest) < k:\n",
    "            add_code((i,code), smallest)\n",
    "        elif len(code) < len(smallest[-1][1]):\n",
    "            smallest.pop()\n",
    "            add_code((i,code), smallest)\n",
    "    return smallest\n",
    "\n",
    "def add_code(code, list):\n",
    "    for i in range(len(list)):\n",
    "        if len(code[1]) < len(list[i][1]):\n",
    "            list.insert(i, code)\n",
    "            #print(f\"Inserted this: {code}\")\n",
    "            return\n",
    "    list.append(code)\n",
    "    #print(f\"Appended this: {code}\")\n",
    "\n",
    "ten_smallest = k_smallest_strings(10, ds['test'])\n",
    "\n",
    "for ex in ten_smallest:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best/Simplest Methods for Reference:\n",
    "* 53: add (two int)\n",
    "* 23: strlen (one string)\n",
    "* 45: traingleArea (two double)\n",
    "* 2: truncateNumber (one double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import PartialState\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-770m\"\n",
    "device = \"cuda:1\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# inputs = tokenizer.encode(ds[2]['prompt'], return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, num_beams = 4, do_sample = True, max_length=1000)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    #quantization_config=bnb_config, #Model quantization\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, #Automatic with quantized models\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    #quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Save model generation values\n",
    "gen_config = model.generation_config\n",
    "config_dict = gen_config.to_dict()\n",
    "with open(\"generation_config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''Sum the elements of an array and return the sum with unit tests following the ZOMBIES testing methodology.\n",
    "Here is a breakdown of the ZOMBIES testing methodology:\n",
    "Z - Zero\n",
    "O - One\n",
    "M - Many\n",
    "B - Boundaries\n",
    "I - Interface\n",
    "E - Exceptions\n",
    "S - Simplicity\n",
    "\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "class Solution {\n",
    "    public double sumArray(List<Double> numbers) {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''//Generate Unit Tests.\n",
    "\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\n",
    "/* Example Test       \n",
    "    @Test\n",
    "    void shouldReturnTrueIfHasClosestElementExample1() {\n",
    "        // Arrange\n",
    "        final var s = new Solution();\n",
    "        final var arg1 = List.of(11.0, 2.0, 3.9, 4.0, 5.0, 2.2);\n",
    "        final var arg2 = 0.3;\n",
    "        final var expected = true;\n",
    "        // Act\n",
    "        final var actual = s.hasCloseElements(arg1, arg2);\n",
    "        \n",
    "        // Assert\n",
    "        assertEquals(expected, actual);\n",
    "    }\n",
    "*/\n",
    "\n",
    "//Include tests for empty and singleton Lists.\n",
    "class SolutionTest {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Solution\",\n",
    "    #package_name=\"com.example.calculator\",\n",
    "    dependencies=[\"JUnit 5\", \"Mockito\"],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Generate Unit Tests.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"\"\"\n",
    "        @Test\n",
    "        void shouldReturnTrueIfHasClosestElementExample1() {\n",
    "            // Arrange\n",
    "            final var s = new Solution();\n",
    "            final var arg1 = List.of(11.0, 2.0, 3.9, 4.0, 5.0, 2.2);\n",
    "            final var arg2 = 0.3;\n",
    "            final var expected = true;\n",
    "\n",
    "            // Act\n",
    "            final var actual = s.hasCloseElements(arg1, arg2);\n",
    "            \n",
    "            // Assert\n",
    "            assertEquals(expected, actual);\n",
    "        }\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "package com.example.calculator;\n",
    "\n",
    "public class Calculator {\n",
    "    public int add(int a, int b) {\n",
    "        return a + b;\n",
    "    }\n",
    "    \n",
    "    public int divide(int numerator, int denominator) {\n",
    "        if (denominator == 0) {\n",
    "            throw new ArithmeticException(\"Cannot divide by zero\");\n",
    "        }\n",
    "        return numerator / denominator;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Calculator\",\n",
    "    package_name=\"com.example.calculator\",\n",
    "    dependencies=[\"JUnit 5\", \"Mockito\"],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Test both the add() and divide() methods\n",
    "    - For divide(), include tests for the divide-by-zero exception\n",
    "    - Test with both positive and negative numbers\n",
    "    - Test with boundary values\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"\"\"\n",
    "        @Test\n",
    "        void shouldReturnSumWhenAddingTwoPositiveNumbers() {\n",
    "            // Arrange\n",
    "            Calculator calculator = new Calculator();\n",
    "            \n",
    "            // Act\n",
    "            int result = calculator.add(3, 5);\n",
    "            \n",
    "            // Assert\n",
    "            assertEquals(8, result, \"3 + 5 should equal 8\");\n",
    "        }\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_prompt)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strlen Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "\n",
    "# Set up Jinja environment\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"java_unit_test_template.j2\")\n",
    "\n",
    "# Example Java class to test\n",
    "java_class = \"\"\"\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "\n",
    "class Solution {\n",
    "    public int strlen(String string) {\n",
    "        return string.length();\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# Render the template with context\n",
    "formatted_prompt = template.render(\n",
    "    source_code=java_class,\n",
    "    class_name=\"Solution\",\n",
    "    #package_name=\"com.example.calculator\",\n",
    "    dependencies=[],\n",
    "    specific_requirements=\"\"\"\n",
    "    - Generate Unit Tests.\n",
    "    - Test for Empty Cases\n",
    "    - Test for Failing Cases\n",
    "    - Test for Error Causing Cases\n",
    "    \"\"\",\n",
    "    examples=[],\n",
    "    test_framework = \"Any\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['declaration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['canonical_solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['test'][23]['prompt'] + ds['test'][23]['canonical_solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt + \"\\n\\n\\n//Source method\\n\" + remove_block_comments(ds['test'][23]['prompt']) + ds['test'][23]['canonical_solution'] + \"\\n\\n//Unit Tests with Three Cases\\n\" + \"public class Main {\\n    public static void main(String[] args) {\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"//Source method\\n\" + remove_block_comments(ds['test'][45]['prompt']) + ds['test'][45]['canonical_solution'] + \"\\n\\n//Unit Tests with Three Cases\\n\" + ds['test'][45]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"//Source method\\nClass Solution{\" + ds['test'][45]['canonical_solution'] + \"\\n\\n//Unit Tests with Three Cases\\n\" + ds['test'][45]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_block_comments(java_code):\n",
    "    \"\"\"\n",
    "    Removes all block comments (/* ... */) from the given Java code.\n",
    "    \n",
    "    Args:\n",
    "        java_code (str): The Java code as a string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The Java code with block comments removed.\n",
    "    \"\"\"\n",
    "    # Regex pattern to match block comments\n",
    "    pattern = re.compile(r'/\\*.*?\\*/', re.DOTALL)  # re.DOTALL makes .* match newlines\n",
    "    \n",
    "    # Substitute block comments with empty string\n",
    "    cleaned_code = re.sub(pattern, '', java_code)\n",
    "    \n",
    "    return cleaned_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_block_comments(java_code):\n",
    "    pattern = re.compile(r'/\\*.*?\\*/\\s*\\n?', re.DOTALL)  # Finds block and newline character after\n",
    "    \n",
    "    # Substitute block comments (and following newline) with empty string\n",
    "    cleaned_code = re.sub(pattern, '', java_code)\n",
    "    \n",
    "    return cleaned_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_block_comments(ds['test'][23]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/media/mujtaba/DATA/nick/UnitTestExamples/data/prompts/humanevalx/few_shot_first_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = df.iloc[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public double triangleArea(double a, double h) {\n",
      "        return a * h / 2;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Test Cases\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.triangleArea(5, 3) == 7.5,\n",
      "                s.triangleArea(2, 2) == 2.0,\n",
      "                s.triangleArea(10, 8) == 40.0\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Test Cases\n",
      "public class Main {\n",
      "        public static void main(String[] args) {\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public double triangleArea(double a, double h) {\n",
      "        return a * h / 2;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Test Cases\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.triangleArea(5, 3) == 7.5,\n",
      "                s.triangleArea(2, 2) == 2.0,\n",
      "                s.triangleArea(10, 8) == 40.0\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int strlen(String string) {\n",
      "        return string.length();\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Test Cases\n",
      "public class Main {\n",
      "        public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.strlen(\"hello\") == 5,\n",
      "                s.strlen(\"hello world\") == 11,\n",
      "                s.strlen(\"123456789\") == 9\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int sum(int a, int b) {\n",
      "        return a + b;\n",
      "    }\n",
      "}\n",
      "\n",
      "//Unit Test Cases\n",
      "public class Main {\n",
      "        public static void main(String[] args) {\n",
      "        Solution s = new Solution();\n",
      "        List<Boolean> correct = Arrays.asList(\n",
      "                s.sum(1, 2) == 3,\n",
      "                s.sum(10, 20) == 30,\n",
      "                s.sum(100, 200) == 300\n",
      "        );\n",
      "        if (correct.contains(false)) {\n",
      "            throw new AssertionError();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "//Source method\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "\n",
      "class Solution {\n",
      "    public int sum(int a, int\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "outputs = model.generate(inputs, max_new_tokens=300)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "outputs = model.generate(inputs, \n",
    "                         \n",
    "                         ####OUTPUT LENGTH CONTROL####\n",
    "                         #max_length = 1200, #optional, default = 20, max length of tokens in input prompt + newly generated tokens\n",
    "                         max_new_tokens = 300, #optional, max number of model generated tokens (not including prompt)\n",
    "                         min_length = 0, #optional, default = 0, min length of tokens in input prompt + newly generated tokens\n",
    "                         #min_new_tokens, #optional, min number of model generated tokens (not including prompt)\n",
    "                         early_stopping = False, #optional, default = False, True: stops with num_beams completion candidates; False: Stops when better candidates are unlikely; \"never\": only stops when there cannot be any better candidates\n",
    "                         max_time = 120, #optional (in seconds)\n",
    "                         #stop_strings, #optional (string or list of strings that terminate generation)\n",
    "                         \n",
    "                         ####MANIPULATION OF MODEL OUTPUT LOGITS####\n",
    "                         temperature = 1.5, #optional, default = 1.0, >1.0: more random; <1.0: more controlled/repetitive; 1.0: balance between both (check generation.config file for default values)\n",
    "                         top_k = 50, #optional, default = 50, considers the k most probable tokens for generation\n",
    "                         top_p = 1, #optional, default = 1.0, cumulative probability of all top_k tokens needed; if set <1.0, only the number of tokens that achieve that probability are considered (check generation.config for alterations)\n",
    "                         #min_p = 0.01, #optional, minimum token probability for consideration, scaled based on the most likely token\n",
    "                         typical_p = 1.0, #optional, default = 1.0, ?\n",
    "                         epsilon_cutoff = 0.0, #optional, default = 0.0, tokens must have a conditional probability greater than the value (3e-4 - 9e-4)\n",
    "                         eta_cutoff = 0.0, #optional, default = 0.0, hybrid between epsilon and typical p sampling (3e-4 - 2e-3)\n",
    "                         diversity_penalty = 2.0, #optional, default = 0.0, value subtracted from beam score if another beam generates the same token; group beam search must be enabled\n",
    "                         repetition_penalty = 10.0, #optional, default = 1.0, 1.0: no penalty, ****need to experiment with values****\n",
    "                         #encoder_repitition_penalty = 1.0, #optional, default = 1.0, exponential penalty on sequences not in the original input\n",
    "                         length_penalty = 0.0, #optional, default = 1.0, exponential penalty to length; <0.0 means shorter sequences; >0.0 means longer sequences\n",
    "                         no_repeat_ngram_size = 0, #optional, default = 0, if set, ngrams of that size can never be repeated once used\n",
    "                         #bad_words_ids, #optional, tokens that are prevented from being generated\n",
    "                         #force_word_ids, #optional, list of token ids that must be generated\n",
    "                         renormalize_logits = True, #optional, default = False, whether logits are renormalized,recommended to be set to True as logit processors can break normalization\n",
    "                         #constraints, #optional, list of constraints added to generation to promote usage of certain tokens\n",
    "                         forced_bos_token_id = model.config.forced_bos_token_id, #optional, defaults to model.config.forced_bos_token_id, this is the token forced to be the first generation\n",
    "                         forced_eos_token_id = model.config.forced_eos_token_id, #optional, default = model.config.forced_eos_token_id, this is the token forced to be the last generation\n",
    "                         remove_invalid_values = model.config.remove_invalid_values, #optional, defaults to model.config.remove_invalid_values, if true, removes nan and inf outputs to prevent crash, but slows down generation\n",
    "                         #exponential_decay_length_penalty, #optional, given tuple that adds increasing penalty after the given number of tokens are generated tuple(int(num tokens), float(penalty))\n",
    "                         #suppress_tokens, #optional, list of tokens suppressed at generation; log probs set to inf\n",
    "                         #begin_suppress_tokens, #optional, list of tokens suppressed at beginning of generation; log probs set to inf\n",
    "                         #forced_decoder_ids, #optional, list of generation indicies to token indicies that will be forced before sampling begins\n",
    "                         #sequence_bias, #optional, dictionary mapping sequences to bias terms; positive bias = increased odds, negative bias = decreased odds\n",
    "                         token_healing = False, #optional, default = False\n",
    "                         #guidance_scale, #optional\n",
    "                         #low_memory, #optional\n",
    "                         #watermarking_config, #optional\n",
    "\n",
    "                         ####GENERATION STRATEGY####\n",
    "                         do_sample = False, #optional, default = False, whether sampling is used; greedy decoding if False\n",
    "                         num_beams = 8, #optional, default = 1, number of beams used for beam search; 1 means no beam search\n",
    "                         num_beam_groups = 4, #optional, default = 1, number of beam groups that beams divide into to ensure diversity among beam groups\n",
    "                         #penalty_alpha, #optional, balances model confidence and degeneration penalty in contrastive search decoding\n",
    "                         #dola_layers, #optional, number of layers used for DoLa decoding (LOOK INTO MORE THROUGH DOCS AND PAPER)\n",
    "\n",
    "                         ####SPECIAL TOKENS AT GENERATION TIME####\n",
    "                         pad_token_id = tokenizer.eos_token_id, #optional, id of padding token\n",
    "                         #bos_token_id, #optional, id of beginning-of-sequence token\n",
    "                         #eos_token_id, #optional, id of end-of-sequence token\n",
    "\n",
    "                         ####CACHE CONTROL####\n",
    "                         use_cache = True, #optional, default = True, whether model uses past key/values attentions to speed up decoding\n",
    "                         cache_implementation = None, #optional, default = None, type of cache implementation used\n",
    "                         cache_config = None, #optional, default = None, \n",
    "                         return_legacy_cache = True, #optional, default = True                   \n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SantaCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/santacoder\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCoder2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoder2-3b\"\n",
    "device = \"cuda:2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(ds[0]['prompt'], return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, \n",
    "                         max_new_tokens=400,\n",
    "                         top_k = 0.7)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codegen2-1B_P\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(\"//Create a function that sums two numbers together and returns the result\\npublic static int sum(int a, int b){\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Salesforce/codegen25-7b-instruct:\n",
      "- tokenization_codegen25.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "add_special_tokens conflicts with the method add_special_tokens in CodeGen25Tokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/codegen25-7b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/codegen25-7b-instruct_P\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:919\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[1;32m    918\u001b[0m         tokenizer_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2036\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2034\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2276\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2276\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2278\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2281\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Salesforce/codegen25-7b-instruct/5c3821aa6090f1cada356c5c60fe96cf68b33945/tokenization_codegen25.py:136\u001b[0m, in \u001b[0;36mCodeGen25Tokenizer.__init__\u001b[0;34m(self, pad_token, eos_token, add_eos_token, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m pad_token_added \u001b[38;5;241m=\u001b[39m AddedToken(pad_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[1;32m    135\u001b[0m eos_token_added \u001b[38;5;241m=\u001b[39m AddedToken(eos_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eos_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m eos_token\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_added\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_added\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m tiktoken_tokenizer(base\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, pad_token\u001b[38;5;241m=\u001b[39mpad_token, add_special\u001b[38;5;241m=\u001b[39madd_special_tokens)\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/tokenization_utils.py:435\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mcontent: v \u001b[38;5;28;01mfor\u001b[39;00m v, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# 4 init the parent class\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_tokens(\n\u001b[1;32m    440\u001b[0m     [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens_extended \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder],\n\u001b[1;32m    441\u001b[0m     special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1407\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)):\n\u001b[0;32m-> 1407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m conflicts with the method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: add_special_tokens conflicts with the method add_special_tokens in CodeGen25Tokenizer"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen25-7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen25-7b-instruct_P\", trust_remote_code=True).to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeT5+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-2b\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Need to Pass Generation Config for CodeT5+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "gen_config = GenerationConfig(max_new_tokens = 300)\n",
    "outputs = model.generate(inputs, generation_config=gen_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    #quantization_config=bnb_config, #Model quantization\n",
    "    trust_remote_code=True,\n",
    "    #low_cpu_mem_usage=True, #Automatic with quantized models\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek Coder V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_4bit=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    load_in_8bit=True,\n",
    "    # bnb_8bit_quant_type=\"nf8\",\n",
    "    # bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", \n",
    "                                             trust_remote_code=True,\n",
    "                                             quantization_config = bnb_config\n",
    "                                             )#.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-base\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Salesforce/codet5-base\n",
      "Total parameters: 222,882,048\n",
      "Approx. size: 850.23 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "size_in_bytes = total_params * 4  # assuming 32-bit (float32), which is 4 bytes\n",
    "size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "print(f\"Model: Salesforce/codet5-base\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Approx. size: {size_in_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitTestGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
